{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b069e8e-a8d5-4cc0-a2b0-c15f28a795fd",
   "metadata": {},
   "source": [
    "## Instantiate Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b2b0894-3db2-440c-975c-65ae6dab4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"process_bronze\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\")\\\n",
    "        .config(\"spark.executor.instances\", \"1\")\\\n",
    "        .config(\"spark.driver.memory\", \"700m\")\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56730c-2205-41c4-bf7b-2ba6b822b320",
   "metadata": {},
   "source": [
    "## Variables and Imports\n",
    "Here comes some common variables since we wold not like to repeat ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4814396b-c6e4-40ed-aba6-8968ef63d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import length, trim, coalesce, lit, udf, count, col, row_number, date_format, avg\n",
    "from pyspark.sql.window import Window\n",
    "data_path = \"/home/jovyan/data/bronze/demo/*/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5948e1ce-8e74-43e5-a239-d331b1fcb848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.udfs import is_null_or_empty, is_not_null_nor_empty, trim_and_lower_case\n",
    "\n",
    "is_null_or_empty_udf = udf(is_null_or_empty, BooleanType())  \n",
    "is_not_null_nor_empty_udf = udf(is_not_null_nor_empty, BooleanType())\n",
    "trim_and_lower_case_udf = udf(trim_and_lower_case, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326d83f-f471-4190-8ac6-274ba04e2f96",
   "metadata": {},
   "source": [
    "# Task 1: Familiarize yourself with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3954827-121c-402b-ba90-a449df1f320e",
   "metadata": {},
   "source": [
    "## Describe Data\n",
    "Let's see what the data consists of to get a better understanding of schema, null values, nullable columns etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "801c8045-ed41-4f7c-82fa-adc594f88381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+-------------------+----------+--------------------+---------------+------------------+--------------------+\n",
      "|summary|            DeviceID|       DataSourceID|             SiteID|DeviceType|           Timestamp|         Metric|             Value| ProcessingTimestamp|\n",
      "+-------+--------------------+-------------------+-------------------+----------+--------------------+---------------+------------------+--------------------+\n",
      "|  count|              797977|             797977|             797977|    797977|              797977|         797977|            797977|              797977|\n",
      "|   mean| 2.178606038862169E9|2.654070997002039E9|3.343580468943778E9|      NULL|                NULL|           NULL|  25109.5576413759|                NULL|\n",
      "| stddev|1.1426866835683913E9|1.197406648356499E9|1.636059864016143E9|      NULL|                NULL|           NULL|339067.17024964694|                NULL|\n",
      "|    min|          1003166906|         1020791452|         4268260543|  DcString|2021-10-31T02:00:...|ac_active_power|              -0.0|2021-11-01T00:01:...|\n",
      "|    max|           976699438|          843004523|          448861961|    Sensor|2021-11-08T00:30:...|     wind_speed|           9994.03|2021-11-07T23:45:...|\n",
      "+-------+--------------------+-------------------+-------------------+----------+--------------------+---------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"header\", True).csv(data_path).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6dd1c-2c34-4bf6-818b-831a6c39f7a0",
   "metadata": {},
   "source": [
    "## Read the Data\n",
    "Let's enforce schema instead of infering it. \n",
    "- Clearly DeviceId, DataSourceID, SiteID must be strings because we won't need any arithmetical operations on them. These columns ideally must not be null, at least DeviceID\n",
    "- DeviceType and Metric are also need to be string. They need to be checked for null values\n",
    "- Timestamp and ProcessingTimestamp are TimestampType, also need to be checked for null values\n",
    "- Value must be double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75fb5b37-7b73-4800-b472-d17b88c514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([\n",
    "        StructField(\"DeviceID\", StringType(), False),\n",
    "        StructField(\"DataSourceID\", StringType(), False),\n",
    "        StructField(\"SiteID\", StringType(), False),\n",
    "        StructField(\"DeviceType\", StringType(), True),\n",
    "        StructField(\"Timestamp\", TimestampType(), True),\n",
    "        StructField(\"Metric\", StringType(),True),\n",
    "        StructField(\"Value\", DoubleType(), True),\n",
    "        StructField(\"ProcessingTimestamp\", TimestampType(), True)\n",
    "])\n",
    "raw_df = spark.read.option(\"header\", True).schema(data_schema).csv(data_path).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a5b2bc-7219-4104-ba4c-6d7f3fb52079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----------+-------------------+------------+-----+-----------------------+\n",
      "|DeviceID  |DataSourceID|SiteID    |DeviceType|Timestamp          |Metric      |Value|ProcessingTimestamp    |\n",
      "+----------+------------+----------+----------+-------------------+------------+-----+-----------------------+\n",
      "|3137912842|843004523   |4268260543|Sensor    |2021-11-06 23:00:48|digital_in_1|1.0  |2021-11-07 06:38:56.654|\n",
      "|3137912842|843004523   |4268260543|Sensor    |2021-11-06 23:00:48|digital_in_2|1.0  |2021-11-07 06:38:56.654|\n",
      "|3137912842|843004523   |4268260543|Sensor    |2021-11-06 23:00:48|digital_in_3|0.0  |2021-11-07 06:38:56.654|\n",
      "|3137912842|843004523   |4268260543|Sensor    |2021-11-06 23:00:48|digital_in_4|1.0  |2021-11-07 06:38:56.654|\n",
      "|3137912842|843004523   |4268260543|Sensor    |2021-11-06 23:00:48|digital_in_5|0.0  |2021-11-07 06:38:56.654|\n",
      "+----------+------------+----------+----------+-------------------+------------+-----+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c85262-2487-4dbe-b285-69f3ff71b950",
   "metadata": {},
   "source": [
    "# Task 2: Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261b9ed-a451-4865-882e-d6202c65b462",
   "metadata": {},
   "source": [
    "## Rename Columns\n",
    "Rename columns for redeability purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a205036e-93a5-49da-951e-528848e1794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.select(col(\"DeviceID\").alias(\"device_id\"), \n",
    "                   col(\"DataSourceID\").alias(\"data_source_id\"), \n",
    "                   col(\"SiteID\").alias(\"site_id\"), \n",
    "                   col(\"DeviceType\").alias(\"device_type\"),\n",
    "                   col(\"Timestamp\").alias(\"event_timestamp\"), \n",
    "                   col(\"Metric\").alias(\"metric\"),\n",
    "                   col(\"Value\").alias(\"value\"),\n",
    "                   col(\"ProcessingTimestamp\").alias(\"processing_timestamp\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814bd5e-3c71-4040-900a-90695520227d",
   "metadata": {},
   "source": [
    "## Data Quality Checks\n",
    "\n",
    "Let's define some rules\n",
    "- Q1: Keep events which don't have null or empty columns\n",
    "- Q2: event_timestamp can not be greater then processing_timestamp. The data processed in the system is logically has happened in the past. So keep events that has correct timepstamps\n",
    "- Q3: There should be only one value for a metric calculated at a certain time, otherwise it is a duplicate. We have to apply a window over dataframe to keep the event with last processing time in case duplicates\n",
    "\n",
    "Apply this filters to clean raw data as parquet and keep the wrong records as csv for future corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba50c7-2de1-4811-85e3-fcc42d915264",
   "metadata": {},
   "source": [
    "### Implement Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d53013b-4eb8-4cfe-be37-0b1c13259ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: not empty nor null values\n",
    "from functools import reduce\n",
    "filter_not_empty_nor_null_q1 = reduce(lambda a, b: a & b,[is_not_null_nor_empty_udf(col(c)) for c in df.columns])\n",
    "\n",
    "#Q2: processing_timestamp must be greater or equal to (maybe differs by nanoseconds?) then event_timestamp .\n",
    "filter_correct_date_q2 = col(\"processing_timestamp\") >= col(\"event_timestamp\")\n",
    "\n",
    "#Q3: if there is a duplicate on columns \"device_id\", \"data_source_id\", \"site_id\", \"device_type\", \"event_timestamp\", \"metric\", \"value\" then keep the last processed measure\n",
    "window_last_processed_q3 = Window.partitionBy(\"device_id\", \"data_source_id\", \"site_id\", \"device_type\", \"event_timestamp\", \"metric\", \"value\").orderBy(col(\"processing_timestamp\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec70daf-f867-4ce7-8ebf-eeef2cca19a8",
   "metadata": {},
   "source": [
    "### Write clean data \n",
    "Write clean data as parquet files in silver layer by partitioning device_id and processiong date. \\\n",
    "The data is saved in data/silver/demo \\\n",
    "Since we are in a single node cluster no cloasce needed to reduce small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf9437be-356c-43f9-9d32-f848ff6c8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final dataframe we should work on it and save\n",
    "clean_df = df.filter(filter_not_empty_nor_null_q1) \\\n",
    "        .filter(filter_correct_date_q2) \\\n",
    "        .withColumn(\"row_number\", row_number().over(window_last_processed_q3)) \\\n",
    "        .filter(col(\"row_number\") == 1) \\\n",
    "        .drop(\"row_number\").cache()\n",
    "\n",
    "clean_df.withColumn(\"Date\", date_format(col(\"processing_timestamp\"), \"yyyy-MM-dd\")) \\\n",
    "        .write.partitionBy(\"Date\") \\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .parquet(\"/home/jovyan/data/silver/demo/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b3541-9a85-4d57-ac21-a3b512928739",
   "metadata": {},
   "source": [
    "### Save Corrupt results\n",
    "Save corrupt records in csv format in case they are wanted to be corrected \\\n",
    "The data is saved in data/corrupt/demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e698839b-7423-4a71-bc3b-129ae43e3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dataframe we should save in another location for further analysis and correction if needed. Better to save then sorry\n",
    "corrupt_df = df.exceptAll(clean_df)\n",
    "corrupt_df.withColumn(\"Date\", date_format(col(\"processing_timestamp\"), \"yyyy-MM-dd\")) \\\n",
    "    .write.partitionBy(\"Date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"/home/jovyan/data/corrupt/demo/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59c334-5238-4050-a1a7-9de665397789",
   "metadata": {},
   "source": [
    "### Task 6: Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9387939-5d16-4e44-b747-a4da0e9c4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some checks:\n",
    "assert clean_df.count() + corrupt_df.count() == raw_df.count(), \"Total number of clean and corrupt events does not add up to total number of events\"\n",
    "assert clean_df.count() == clean_df.dropDuplicates().count(), \"Cleaned dataframe has duplicates\"\n",
    "assert clean_df.filter(filter_not_empty_nor_null_q1).count() == clean_df.count(), \"Cleaned dataframe has null or empty values\"\n",
    "assert clean_df.filter(filter_correct_date_q2).count() == clean_df.count(), \"Cleaned dataframe has incorrect dates\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217532d7-16d9-4aa5-9263-834c44b2a677",
   "metadata": {},
   "source": [
    "# Task 3: Compute the \"Inverter AC Hourly Yield\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fabbcb-ad91-4df7-ad7f-5b0a15642357",
   "metadata": {},
   "source": [
    "There are 4 measures done in an hour which makes sense since they are done each 15 mins, so there must be 4 data points with average 39206.5325\n",
    "```python\n",
    "clean_df.withColumn(\"event_hour\", date_format(col(\"event_timestamp\"), \"yyyy-MM-dd HH:00\")).filter((col(\"device_id\") == 1054530426)\n",
    "                & (trim_and_lower_case_udf(col(\"device_type\")) == \"inverter\")\n",
    "                & (trim_and_lower_case_udf(col(\"metric\")) == \"ac_active_power\")\n",
    "                & (col(\"event_hour\") == \"2021-11-04 11:00\")).show()\n",
    "+----------+--------------+----------+-----------+-------------------+---------------+--------+--------------------+----------------+\n",
    "| device_id|data_source_id|   site_id|device_type|    event_timestamp|         metric|   value|processing_timestamp|      event_hour|\n",
    "+----------+--------------+----------+-----------+-------------------+---------------+--------+--------------------+----------------+\n",
    "|1054530426|     843004523|4268260543|   Inverter|2021-11-04 11:00:31|ac_active_power|55342.67|2021-11-04 12:41:...|2021-11-04 11:00|\n",
    "|1054530426|     843004523|4268260543|   Inverter|2021-11-04 11:15:35|ac_active_power|35217.33|2021-11-04 12:41:...|2021-11-04 11:00|\n",
    "|1054530426|     843004523|4268260543|   Inverter|2021-11-04 11:30:35|ac_active_power|25806.67|2021-11-04 12:41:...|2021-11-04 11:00|\n",
    "|1054530426|     843004523|4268260543|   Inverter|2021-11-04 11:45:37|ac_active_power|40459.46|2021-11-04 12:41:...|2021-11-04 11:00|\n",
    "+----------+--------------+----------+-----------+-------------------+---------------+--------+--------------------+----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ad6f062-6bec-47b4-8f77-f761435fd724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+----------------+----------+----------+\n",
      "|device_type|         metric| device_id|      event_hour| avg_value|num_points|\n",
      "+-----------+---------------+----------+----------------+----------+----------+\n",
      "|   Inverter|ac_active_power|1054530426|2021-11-04 11:00|39206.5325|         4|\n",
      "+-----------+---------------+----------+----------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_df = clean_df.withColumn(\"event_hour\", date_format(col(\"event_timestamp\"), \"yyyy-MM-dd HH:00\"))\\\n",
    "        .groupBy(\"device_type\", \"metric\", \"device_id\", \"event_hour\").agg(avg(\"value\").alias(\"avg_value\"), count(\"*\").alias(\"num_points\")).cache()\n",
    "avg_inverter_df = avg_df.filter((col(\"device_id\") == 1054530426) & (col(\"event_hour\") == \"2021-11-04 11:00\")\n",
    "              & (trim_and_lower_case_udf(col(\"device_type\")) == \"inverter\") \n",
    "              & (trim_and_lower_case_udf(col(\"metric\")) == \"ac_active_power\") )\n",
    "avg_inverter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fdc0abb-dd5f-4140-9517-e8af09e0b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert avg_inverter_df.select(\"num_points\").first()[\"num_points\"] == 4, \"number of data points does not match\"\n",
    "assert avg_inverter_df.select(\"avg_value\").first()[\"avg_value\"] == 39206.5325, \"average value does not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dcea5-31ec-4cda-8bbb-1f37f1a0ed5d",
   "metadata": {},
   "source": [
    "# Task 4: Compute the \"Satellite/Sensor Hourly Irradiance\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e4527-4299-46eb-8790-48e689c7fa57",
   "metadata": {},
   "source": [
    "There are 4 measures done in an hour for Satellite, so there must be 4 data points with average 49.415. However, there are no sensor data for this device, not even in raw_df\n",
    "```python\n",
    "clean_df.withColumn(\"event_hour\", date_format(col(\"event_timestamp\"), \"yyyy-MM-dd HH:00\")).filter((col(\"device_id\") == 3258837907)\n",
    "                & (trim_and_lower_case_udf(col(\"metric\")) == \"irradiance\")\n",
    "                & (col(\"event_hour\") == \"2021-11-04 11:00\")).show()\n",
    "+----------+--------------+----------+-----------+-------------------+----------+-----+--------------------+----------------+\n",
    "| device_id|data_source_id|   site_id|device_type|    event_timestamp|    metric|value|processing_timestamp|      event_hour|\n",
    "+----------+--------------+----------+-----------+-------------------+----------+-----+--------------------+----------------+\n",
    "|3258837907|    2805564098|4268260543|  Satellite|2021-11-04 11:00:00|irradiance| 69.6|2021-11-05 09:45:...|2021-11-04 11:00|\n",
    "|3258837907|    2805564098|4268260543|  Satellite|2021-11-04 11:15:00|irradiance|31.11|2021-11-05 10:00:...|2021-11-04 11:00|\n",
    "|3258837907|    2805564098|4268260543|  Satellite|2021-11-04 11:30:00|irradiance| 17.5|2021-11-05 10:15:...|2021-11-04 11:00|\n",
    "|3258837907|    2805564098|4268260543|  Satellite|2021-11-04 11:45:00|irradiance|79.45|2021-11-05 10:30:...|2021-11-04 11:00|\n",
    "+----------+--------------+----------+-----------+-------------------+----------+-----+--------------------+----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1d2a7e3-2ec0-4458-9cc8-f1d525c09307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------------+---------+----------+\n",
      "|device_type|    metric| device_id|      event_hour|avg_value|num_points|\n",
      "+-----------+----------+----------+----------------+---------+----------+\n",
      "|  Satellite|irradiance|3258837907|2021-11-04 11:00|   49.415|         4|\n",
      "+-----------+----------+----------+----------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_sat_sens_df = avg_df.filter((col(\"device_id\") == 3258837907) & (col(\"event_hour\") == \"2021-11-04 11:00\")\n",
    "              & (trim_and_lower_case_udf(col(\"device_type\")).isin(\"satellite\", \"sensor\"))\n",
    "              & (trim_and_lower_case_udf(col(\"metric\")) == \"irradiance\") )\n",
    "avg_sat_sens_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f33a1d80-8001-470a-bcb1-b99c95c32f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert avg_sat_sens_df.select(\"num_points\").first()[\"num_points\"] == 4, \"number of data points does not match\"\n",
    "assert avg_sat_sens_df.select(\"avg_value\").first()[\"avg_value\"] == 49.415, \"average value does not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd8f96-baad-47d6-b3e3-9a93e9b1a04d",
   "metadata": {},
   "source": [
    "# Task 5: Store the result\n",
    "I have used device_type and metric columns as partitions. Using date is not a good idea since 4 * 24 = 96 records for a device_id + metric + date is too small.\n",
    "Usually the records will be queried by device_type and metric so partitioning by those two is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5396dd37-1e42-4763-b580-ff5b9b0fc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df.withColumn(\"device_type\", trim(col(\"device_type\")))\\\n",
    "    .withColumn(\"metric\", trim(col(\"metric\")))\\\n",
    "    .write.partitionBy(\"device_type\", \"metric\").mode(\"overwrite\").parquet(\"/home/jovyan/data/gold/demo/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
